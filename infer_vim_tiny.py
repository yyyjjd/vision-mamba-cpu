import torch
from torchvision import transforms
from fvcore.nn import FlopCountAnalysis
import time
import sys
import os

# 1. å¯¼å…¥æ¨¡å‹æ³¨å†Œå‡½æ•°
sys.path.append(os.path.dirname(__file__))  # ä¿è¯èƒ½importæœ¬åœ°æ¨¡å—
from models_mamba import vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2

# é…ç½®
CKPT_PATH = 'vim_t_midclstok_76p1acc.pth'  # ä½ çš„æƒé‡è·¯å¾„
DEVICE = 'cpu'  # æ ‘è“æ´¾å»ºè®®ç”¨cpu
BATCH_SIZE = 8
NUM_IMAGES = 100  # å¯æ”¹ä¸º1000
input_size = 224

# 2. åŠ è½½æ¨¡å‹
model = vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(num_classes=1000)
model.eval()
model.to(DEVICE)
ckpt = torch.load(CKPT_PATH, map_location=DEVICE, weights_only=False)
if 'model' in ckpt:
    ckpt = ckpt['model']
state_dict = model.state_dict()
model.load_state_dict(ckpt, strict=True)

# 3. ç”Ÿæˆéšæœºå›¾ç‰‡æ•°æ®
all_imgs = torch.randn(NUM_IMAGES, 3, input_size, input_size)

# 4. FLOPs ç»Ÿè®¡ï¼ˆåªéœ€ä¸€æ¬¡ï¼Œå–ä¸€å¼ å›¾å³å¯ï¼‰
# flop_inputs = all_imgs[0:1].to(DEVICE)
# flops = FlopCountAnalysis(model, flop_inputs)
# print(f"Single image FLOPs: {flops.total() / 1e9:.2f} GFLOPs")

# æ›´ç²¾å‡†çš„ç†è®º FLOPs ä¼°ç®—ï¼ˆvim-tinyï¼Œ224x224è¾“å…¥ï¼‰
# img_size = 224
# patch_size = 16
# embed_dim = 192
# depth = 24
# d_state = 16
# num_classes = 1000
#
# num_patches = (img_size // patch_size) ** 2
#
# # Patch Embedding
# patch_embed_flops = 2 * embed_dim * num_patches * 3 * patch_size * patch_size
#
# # SelectiveScan/SSM
# ssm_flops_per_layer = 2 * embed_dim * num_patches * d_state
# ssm_total_flops = ssm_flops_per_layer * depth
#
# # MLP
# mlp_hidden_dim = 4 * embed_dim
# mlp_flops_per_layer = (
#     2 * num_patches * (embed_dim * mlp_hidden_dim) +  # ç¬¬ä¸€å±‚å…¨è¿æ¥
#     num_patches * mlp_hidden_dim +                   # æ¿€æ´»
#     2 * num_patches * (mlp_hidden_dim * embed_dim)   # ç¬¬äºŒå±‚å…¨è¿æ¥
# )
# mlp_total_flops = mlp_flops_per_layer * depth
#
# # LayerNorm
# layernorm_flops_per_layer = 4 * num_patches * embed_dim
# layernorm_total_flops = layernorm_flops_per_layer * depth * 2  # é€šå¸¸æ¯å±‚2æ¬¡LayerNorm
#
# # Residual Add
# residual_flops_per_layer = num_patches * embed_dim
# residual_total_flops = residual_flops_per_layer * depth * 2  # é€šå¸¸æ¯å±‚2æ¬¡æ®‹å·®
#
# # Head
# head_flops = 2 * embed_dim * num_classes
#
# # æ€» FLOPs
# total_flops = (
#     patch_embed_flops +
#     ssm_total_flops +
#     mlp_total_flops +
#     layernorm_total_flops +
#     residual_total_flops +
#     head_flops
# )
#
# print(f"[Precise Theoretical Estimate]")
# print(f"Patch Embedding FLOPs: {patch_embed_flops/1e6:.2f} MFLOPs")
# print(f"SelectiveScan FLOPs (total): {ssm_total_flops/1e6:.2f} MFLOPs")
# print(f"MLP FLOPs (total): {mlp_total_flops/1e6:.2f} MFLOPs")
# print(f"LayerNorm FLOPs (total): {layernorm_total_flops/1e6:.2f} MFLOPs")
# print(f"Residual Add FLOPs (total): {residual_total_flops/1e6:.2f} MFLOPs")
# print(f"Head FLOPs: {head_flops/1e6:.2f} MFLOPs")
# print(f"Total FLOPs (theoretical, single 224x224 image): {total_flops/1e9:.2f} GFLOPs\n")

# ç”¨ thop ç»Ÿè®¡æ•´ä¸ªæ¨¡å‹çš„ FLOPs
from thop import profile
from thop import clever_format
import time
import os
import numpy as np

def get_FileSize(filePath):
    filePath = str(filePath)
    fsize = os.path.getsize(filePath)
    fsize = fsize / float(1024 * 1024)
    return round(fsize, 2)

# è®¡ç®—æ¨¡å‹æ–‡ä»¶å¤§å°
mb = get_FileSize(CKPT_PATH)

# ç»Ÿè®¡ FLOPs å’Œå‚æ•°é‡
inputs = torch.randn(1, 3, input_size, input_size)
flops, params = profile(model, (inputs,))

macs, params = clever_format([flops, params], "%.3f")

# ç»Ÿè®¡æ¨ç†æ—¶é—´
start = time.time()
outputs = model(inputs)
infer_time_record = time.time() - start

print('flops: ', macs, ', params: ', params, ', infertime: ', np.mean(infer_time_record), ',mb: ', str(mb)+'MB')

# 4. ç”¨ fvcore ç»Ÿè®¡ FLOPsï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
print("\n=== fvcore FLOPs ç»Ÿè®¡ï¼ˆå¯¹æ¯”éªŒè¯ï¼‰===")
try:
    from fvcore.nn import FlopCountAnalysis
    
    # ä½¿ç”¨ fvcore ç»Ÿè®¡
    flops_fvcore = FlopCountAnalysis(model, inputs)
    total_flops_fvcore = flops_fvcore.total()
    total_params_fvcore = sum(p.numel() for p in model.parameters())
    
    print(f'fvcore FLOPs: {total_flops_fvcore/1000**3:.3f}G')
    print(f'fvcore Params: {total_params_fvcore/1000**2:.1f}M')
    
    # å¯¹æ¯” thop å’Œ fvcore çš„ç»“æœ
    thop_flops_g = float(macs.replace('G', '')) if 'G' in macs else float(macs.replace('M', '')) / 1000
    print(f'\n=== ç»Ÿè®¡ç»“æœå¯¹æ¯” ===')
    print(f'thop FLOPs:     {thop_flops_g:.3f}G')
    print(f'fvcore FLOPs:   {total_flops_fvcore/1e9:.3f}G')
    print(f'å·®å¼‚:           {abs(thop_flops_g - total_flops_fvcore/1000**3):.3f}G')
    print(f'å·®å¼‚æ¯”ä¾‹:       {abs(thop_flops_g - total_flops_fvcore/1000**3)/max(thop_flops_g, total_flops_fvcore/1000**3)*100:.1f}%')
    
    # å¦‚æœ fvcore ç»Ÿè®¡æˆåŠŸï¼Œæ˜¾ç¤ºè¯¦ç»†åˆ†æ
    print(f'\n=== fvcore è¯¦ç»†åˆ†æ ===')
    print("æŒ‰æ¨¡å—ç»Ÿè®¡:")
    print(flops_fvcore.by_module())
    print("\næŒ‰æ“ä½œç±»å‹ç»Ÿè®¡:")
    print(flops_fvcore.by_operator())
    
except ImportError:
    print("âŒ fvcore æœªå®‰è£…ï¼Œè·³è¿‡ fvcore ç»Ÿè®¡")
    print("   å®‰è£…å‘½ä»¤: pip install fvcore")
except Exception as e:
    print(f"âŒ fvcore ç»Ÿè®¡å¤±è´¥: {e}")
    print("   å¯èƒ½åŸå› : æ¨¡å‹åŒ…å«è‡ªå®šä¹‰æ“ä½œï¼Œfvcore æ— æ³•è¯†åˆ«")

# ç†è®ºè®¡ç®— thop ç»Ÿè®¡ä¸åˆ°çš„æ¨¡å— FLOPs
print("\n=== ç†è®º FLOPs è®¡ç®—ï¼ˆthop ç»Ÿè®¡ä¸åˆ°çš„æ¨¡å—ï¼‰===")

# åŒé‡éªŒè¯ï¼šæ—¢ä»æ¨¡å‹è·å–ï¼Œåˆæä¾›ç¡¬ç¼–ç fallback

# è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¨¡å‹çš„æ‰€æœ‰å±æ€§
print("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ¨¡å‹å±æ€§")
model_attrs = [attr for attr in dir(model) if not attr.startswith('_')]
print(f"æ¨¡å‹å±æ€§æ•°é‡: {len(model_attrs)}")
print(f"å‰10ä¸ªå±æ€§: {model_attrs[:10]}")

# æŸ¥æ‰¾å¯èƒ½åŒ…å«å±‚çš„å±æ€§
layer_attrs = [attr for attr in model_attrs if 'layer' in attr.lower() or 'block' in attr.lower()]
print(f"å¯èƒ½çš„å±‚å±æ€§: {layer_attrs}")

try:
    # æ–¹æ³•1ï¼šä»æ¨¡å‹å®ä¾‹ä¸­è·å–å‚æ•°
    img_size = model.patch_embed.img_size[0]
    patch_size = model.patch_embed.patch_size[0]
    embed_dim = model.embed_dim
    num_classes = model.head.out_features
    num_patches = model.patch_embed.num_patches
    
    # å°è¯•è·å–depth - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
    depth = None
    if hasattr(model, 'blocks'):
        depth = len(model.blocks)
    elif hasattr(model, 'layers'):
        depth = len(model.layers)
    elif hasattr(model, 'depth'):
        depth = model.depth
    elif hasattr(model, 'num_layers'):
        depth = model.num_layers
    else:
        # å°è¯•ä»æ¨¡å‹çš„å…¶ä»–å±æ€§æ¨æ–­
        for attr_name in dir(model):
            if 'layer' in attr_name.lower() or 'block' in attr_name.lower():
                attr = getattr(model, attr_name)
                if hasattr(attr, '__len__'):
                    depth = len(attr)
                    break
    
    if depth is None:
        depth = 24  # fallback
    
    # å°è¯•è·å–d_state - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
    d_state = 16  # é»˜è®¤å€¼
    try:
        if hasattr(model, 'blocks') and len(model.blocks) > 0:
            if hasattr(model.blocks[0], 'mixer') and hasattr(model.blocks[0].mixer, 'd_state'):
                d_state = model.blocks[0].mixer.d_state
    except:
        pass
    
    print("âœ… æˆåŠŸä»æ¨¡å‹è·å–å‚æ•°")
    
except Exception as e:
    print(f"âš ï¸ ä»æ¨¡å‹è·å–å‚æ•°å¤±è´¥: {e}")
    print("ğŸ”„ ä½¿ç”¨ç¡¬ç¼–ç å‚æ•°ï¼ˆVim-tiny æ ‡å‡†é…ç½®ï¼‰")
    
    # æ–¹æ³•2ï¼šç¡¬ç¼–ç å‚æ•°ï¼ˆVim-tiny æ ‡å‡†é…ç½®ï¼‰
    img_size = 224
    patch_size = 16
    embed_dim = 192
    depth = 24
    d_state = 16
    num_classes = 1000
    num_patches = (img_size // patch_size) ** 2

# éªŒè¯å‚æ•°åˆç†æ€§
print(f"ğŸ“Š æ¨¡å‹å‚æ•°éªŒè¯:")
print(f"   img_size: {img_size}")
print(f"   patch_size: {patch_size}")
print(f"   embed_dim: {embed_dim}")
print(f"   depth: {depth}")
print(f"   d_state: {d_state}")
print(f"   num_classes: {num_classes}")
print(f"   num_patches: {num_patches}")

# å‚æ•°åˆç†æ€§æ£€æŸ¥
assert img_size == 224, f"img_size åº”è¯¥æ˜¯ 224ï¼Œå®é™…æ˜¯ {img_size}"
assert patch_size == 16, f"patch_size åº”è¯¥æ˜¯ 16ï¼Œå®é™…æ˜¯ {patch_size}"
assert embed_dim == 192, f"embed_dim åº”è¯¥æ˜¯ 192ï¼Œå®é™…æ˜¯ {embed_dim}"
assert depth == 24, f"depth åº”è¯¥æ˜¯ 24ï¼Œå®é™…æ˜¯ {depth}"
assert d_state == 16, f"d_state åº”è¯¥æ˜¯ 16ï¼Œå®é™…æ˜¯ {d_state}"
assert num_patches == 196, f"num_patches åº”è¯¥æ˜¯ 196ï¼Œå®é™…æ˜¯ {num_patches}"

print("âœ… æ‰€æœ‰å‚æ•°éªŒè¯é€šè¿‡ï¼")

# 1. Patch Embedding FLOPs (thop èƒ½ç»Ÿè®¡åˆ°)
patch_embed_flops = 2 * embed_dim * num_patches * 3 * patch_size * patch_size
print(f"Patch Embedding FLOPs: {patch_embed_flops/1e6:.2f} MFLOPs")

# 2. Mamba å— FLOPs (thop ç»Ÿè®¡ä¸åˆ°)
# æ¯ä¸ª Mamba å—åŒ…å«ï¼š
# - å› æœå·ç§¯
# - Selective Scan (SSM) - æ³¨æ„ï¼šBiMamba v2 æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ªå¤„ç†
# - MLP
# - LayerNorm

# å› æœå·ç§¯ FLOPs (BiMamba v2 æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ªå·ç§¯)
conv1d_flops_per_layer = 2 * embed_dim * num_patches * 4  # width=4, åŒå‘å¤„ç†
conv1d_total_flops = conv1d_flops_per_layer * depth

# Selective Scan FLOPs (BiMamba v2 æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ª SSM)
ssm_flops_per_layer = 2 * embed_dim * num_patches * d_state * 2  # åŒå‘å¤„ç†ï¼Œæ‰€ä»¥ä¹˜ä»¥2
ssm_total_flops = ssm_flops_per_layer * depth

# MLP FLOPs (BiMamba v2 çš„å®é™…ç»“æ„)
# ä»æºç åˆ†æï¼šBiMamba v2 æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ªå¤„ç†è·¯å¾„
# æ¯ä¸ªè·¯å¾„åŒ…å«ï¼šx_proj, dt_proj, out_proj

# è¾“å…¥æŠ•å½± (x_proj å’Œ x_proj_b) - åŒå‘å¤„ç†
# x_proj: (batch, seqlen, embed_dim) -> (batch*seqlen, embed_dim*2)
x_proj_flops_per_layer = 2 * num_patches * embed_dim * (embed_dim * 2)  # åŒå‘å¤„ç†ï¼Œexpand=2
x_proj_total_flops = x_proj_flops_per_layer * depth

# æ—¶é—´æ­¥æŠ•å½± (dt_proj å’Œ dt_proj_b) - åŒå‘å¤„ç†
# dt_proj: (batch*seqlen, embed_dim*2) -> (batch*seqlen, dt_rank)
dt_rank = 16  # é»˜è®¤å€¼
dt_proj_flops_per_layer = 2 * num_patches * (embed_dim * 2) * dt_rank  # åŒå‘å¤„ç†
dt_proj_total_flops = dt_proj_flops_per_layer * depth

# è¾“å‡ºæŠ•å½± (out_proj) - åªåœ¨æœ€åç»Ÿä¸€å¤„ç†ï¼Œä¸æ˜¯æ¯å±‚éƒ½æœ‰
out_proj_flops_per_layer = 2 * num_patches * embed_dim * embed_dim  # åŒå‘å¤„ç†
out_proj_total_flops = out_proj_flops_per_layer * depth  # æ¯å±‚éƒ½æœ‰è¾“å‡ºæŠ•å½±

# æ€» MLP FLOPs (åŒ…å«æ¯å±‚çš„è¾“å…¥æŠ•å½±ã€æ—¶é—´æ­¥æŠ•å½±ï¼Œä»¥åŠæ¯å±‚çš„è¾“å‡ºæŠ•å½±)
mlp_total_flops = x_proj_total_flops + dt_proj_total_flops + out_proj_total_flops

# LayerNorm FLOPs
layernorm_flops_per_layer = 4 * num_patches * embed_dim
layernorm_total_flops = layernorm_flops_per_layer * depth * 2  # é€šå¸¸æ¯å±‚2æ¬¡LayerNorm

# æ®‹å·®è¿æ¥ FLOPs
residual_flops_per_layer = num_patches * embed_dim
residual_total_flops = residual_flops_per_layer * depth * 2

# BiMamba v2 ç‰¹æœ‰çš„é¢å¤–è®¡ç®—
# 1. åºåˆ—ç¿»è½¬æ“ä½œ
flip_flops_per_layer = num_patches * embed_dim  # ç¿»è½¬æ“ä½œ
flip_total_flops = flip_flops_per_layer * depth * 2  # å‰å‘å’Œåå‘å„ä¸€æ¬¡

# 2. è¾“å‡ºåˆå¹¶æ“ä½œ
merge_flops_per_layer = num_patches * embed_dim  # åŠ æ³•æ“ä½œ
merge_total_flops = merge_flops_per_layer * depth

# 3. é™¤æ³•æ“ä½œ (if_divide_out=True)
divide_flops_per_layer = num_patches * embed_dim  # é™¤æ³•æ“ä½œ
divide_total_flops = divide_flops_per_layer * depth

# 3. Head FLOPs (thop èƒ½ç»Ÿè®¡åˆ°)
head_flops = 2 * embed_dim * num_classes

# æ€»ç†è®º FLOPs
total_theoretical_flops = (
    patch_embed_flops +
    conv1d_total_flops +
    ssm_total_flops +
    mlp_total_flops +
    layernorm_total_flops +
    residual_total_flops +
    flip_total_flops +      # BiMamba v2 ç¿»è½¬æ“ä½œ
    merge_total_flops +     # BiMamba v2 åˆå¹¶æ“ä½œ
    divide_total_flops +    # BiMamba v2 é™¤æ³•æ“ä½œ
    head_flops
)

print(f"ğŸ“Š FLOPs åˆ†è§£ (ç†è®ºè®¡ç®—):")
print(f"  Patch Embedding: {patch_embed_flops/1e9:.3f}G")
print(f"  å› æœå·ç§¯ (BiMamba v2): {conv1d_total_flops/1e9:.3f}G")
print(f"  Selective Scan (BiMamba v2): {ssm_total_flops/1e9:.3f}G")
print(f"  è¾“å…¥æŠ•å½± (BiMamba v2): {x_proj_total_flops/1e9:.3f}G")
print(f"  æ—¶é—´æ­¥æŠ•å½± (BiMamba v2): {dt_proj_total_flops/1e9:.3f}G")
print(f"  è¾“å‡ºæŠ•å½± (BiMamba v2): {out_proj_total_flops/1e9:.3f}G")
print(f"  LayerNorm: {layernorm_total_flops/1e9:.3f}G")
print(f"  æ®‹å·®è¿æ¥: {residual_total_flops/1e9:.3f}G")
print(f"  åºåˆ—ç¿»è½¬ (BiMamba v2): {flip_total_flops/1e9:.3f}G")
print(f"  è¾“å‡ºåˆå¹¶ (BiMamba v2): {merge_total_flops/1e9:.3f}G")
print(f"  é™¤æ³•æ“ä½œ (BiMamba v2): {divide_total_flops/1e9:.3f}G")
print(f"  Head: {head_flops/1e9:.3f}G")
print(f"  æ€»è®¡: {total_theoretical_flops/1e9:.3f}G")

# å®˜æ–¹é£æ ¼çš„ç®€åŒ– FLOPs è®¡ç®—ï¼ˆåªè®¡ç®—ä¸»è¦æ“ä½œï¼‰
print("\n=== å®˜æ–¹é£æ ¼ç®€åŒ– FLOPs è®¡ç®— ===")

# 1. Patch Embedding (å®˜æ–¹æ ‡å‡†)
official_patch_embed_flops = 2 * embed_dim * num_patches * 3 * patch_size * patch_size

# 2. SSM æ“ä½œ (å®˜æ–¹æ ‡å‡†ï¼Œç®€åŒ–)
official_ssm_flops = 2 * embed_dim * num_patches * d_state * depth

# 3. MLP æ“ä½œ (å®˜æ–¹æ ‡å‡†ï¼Œç®€åŒ–)
official_mlp_flops = 2 * num_patches * embed_dim * embed_dim * depth

# 4. Head (å®˜æ–¹æ ‡å‡†)
official_head_flops = 2 * embed_dim * num_classes

# å®˜æ–¹é£æ ¼æ€» FLOPs
official_total_flops = official_patch_embed_flops + official_ssm_flops + official_mlp_flops + official_head_flops

print(f"ğŸ“Š å®˜æ–¹é£æ ¼ FLOPs åˆ†è§£:")
print(f"  Patch Embedding: {official_patch_embed_flops/1e9:.3f}G")
print(f"  SSM: {official_ssm_flops/1e9:.3f}G")
print(f"  MLP: {official_mlp_flops/1e9:.3f}G")
print(f"  Head: {official_head_flops/1e9:.3f}G")
print(f"  æ€»è®¡ (å®˜æ–¹é£æ ¼): {official_total_flops/1e9:.3f}G")

print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
print(f"  è¯¦ç»†è®¡ç®—: {total_theoretical_flops/1e9:.3f}G")
print(f"  å®˜æ–¹é£æ ¼: {official_total_flops/1e9:.3f}G")
print(f"  å·®å¼‚: {abs(total_theoretical_flops - official_total_flops)/1e9:.3f}G")
print(f"  å·®å¼‚æ¯”ä¾‹: {abs(total_theoretical_flops - official_total_flops)/max(total_theoretical_flops, official_total_flops)*100:.1f}%")

# 5. ååé‡ç»Ÿè®¡
# num_batches = (len(all_imgs) + BATCH_SIZE - 1) // BATCH_SIZE
# start = time.time()
# with torch.no_grad():
#     for i in range(num_batches):
#         batch = all_imgs[i*BATCH_SIZE:(i+1)*BATCH_SIZE].to(DEVICE)
#         _ = model(batch)
# end = time.time()
# total_time = end - start
# throughput = len(all_imgs) / total_time

# print(f"Processed {len(all_imgs)} images in {total_time:.2f} seconds")
# print(f"Average throughput: {throughput:.2f} images/second")
# print(f"Average time per image: {total_time / len(all_imgs):.4f} seconds")
# ååé‡ç»Ÿè®¡å‚æ•°
batch_size = 1
num_images = 1000
log_interval = 10
log_file = "throughput_log.txt"

throughputs = []
start_time = time.time()

with open(log_file, "w") as f:
    for i in range(num_images):
        input_tensor = torch.randn(batch_size, 3, 224, 224)
        t0 = time.time()
        with torch.no_grad():
            _ = model(input_tensor)
        t1 = time.time()
        elapsed = t1 - t0
        throughput = batch_size / elapsed if elapsed > 0 else 0
        throughputs.append(throughput)
        if (i + 1) % log_interval == 0:
            avg_throughput = sum(throughputs[-log_interval:]) / log_interval
            msg = f"Image {i+1}/{num_images}, Avg throughput (last {log_interval}): {avg_throughput:.4f} img/s"
            print(msg)
            f.write(msg + "\n")
    total_time = time.time() - start_time
    overall_avg = sum(throughputs) / len(throughputs)
    summary = f"\nTotal time: {total_time:.2f}s, Overall avg throughput: {overall_avg:.2f} img/s"
    print(summary)
    f.write(summary + "\n")

# ç»Ÿè®¡å‚æ•°é‡
model_param_count = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {model_param_count/1e6:.2f}M")

# ä¿å­˜æ¨¡å‹æƒé‡åˆ°æœ¬åœ°
model_cpu = model.to('cpu')
torch.save(model_cpu.state_dict(), "vim_tiny_cpu.pth")
print("æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ° vim_tiny_cpu.pth") 